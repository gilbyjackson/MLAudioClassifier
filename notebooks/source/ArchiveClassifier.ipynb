{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724abc6a",
   "metadata": {},
   "source": [
    "# Drum Sample Auto-Classifier - Complete Archive Edition (Optimized)\n",
    "\n",
    "This notebook classifies large drum/percussion archives using a trained model while keeping the original archive untouched. Enhanced features include batching, parallel feature extraction, duplicate detection via hashing, optional confidence filtering, flexible copy modes, and reproducible run metadata.\n",
    "\n",
    "## üîç Key Enhancements\n",
    "- **Batch inference** with configurable `BATCH_SIZE`\n",
    "- **Parallel feature extraction** (threaded)\n",
    "- **MD5 hashing for de-duplication** across runs (optional)\n",
    "- **Adjustable output mode**: copy, symlink, or skip materialization\n",
    "- **Confidence threshold filtering**\n",
    "- **Structure preservation** using relative path embedding in output names\n",
    "- **Caching & metadata** (summary, per-file results, error log)\n",
    "- **Resource awareness** (optional system memory display)\n",
    "\n",
    "## üß© Workflow Overview\n",
    "1. Configure paths and parameters.\n",
    "2. Discover audio files recursively (multi-format).\n",
    "3. Preprocess (resample, pad, MFCC extraction).\n",
    "4. Batch predict with loaded Keras model.\n",
    "5. Deduplicate & materialize outputs (optional).\n",
    "6. Summarize + persist metadata for reproducibility.\n",
    "\n",
    "## ‚úÖ Prerequisites\n",
    "Run (in order) before this notebook if retraining:\n",
    "1. `MFCC_Feature_Extractor.ipynb`\n",
    "2. `Model1_Train.ipynb` or `Model2_Train.ipynb`\n",
    "3. (Optional) `Model_Evaluation.ipynb` to compare models.\n",
    "\n",
    "## ‚öôÔ∏è Configuration Highlights\n",
    "| Parameter | Purpose | Example |\n",
    "|-----------|---------|---------|\n",
    "| `ARCHIVE_PATH` | Source root (read-only) | `../complete_drum_archive` |\n",
    "| `RUN_OUTPUT_DIR` | New classified run directory | `../ClassifiedArchive/run_<ts>` |\n",
    "| `COPY_MODE` | Output behavior | `copy | symlink | none` |\n",
    "| `DEDUP_HASH` | Skip files with seen content | `True` |\n",
    "| `CONFIDENCE_THRESHOLD` | Minimum probability to emit | `0.0‚Äì1.0` |\n",
    "| `BATCH_SIZE` | Prediction batch size | `32` |\n",
    "| `INSTRUMENT_NAMES` | Class label mapping | `[Crash,...,Tom]` |\n",
    "\n",
    "## üì¶ Outputs\n",
    "Each run creates:\n",
    "- Classified files per instrument folder (unless COPY_MODE = none)\n",
    "- `metadata/summary.json` (run-wide stats & config)\n",
    "- `metadata/results.json` (per-file predictions)\n",
    "- `metadata/errors.json` (only if failures occurred)\n",
    "- Updated hash cache for dedupe if enabled.\n",
    "\n",
    "## üîÅ Reproducibility\n",
    "Changing parameters yields a new timestamped run directory, preserving previous results. Hash-based deduplication prevents redundant processing between runs if the same file content reappears.\n",
    "\n",
    "---\n",
    "Proceed to the next cell to load dependencies and configure runtime parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033fed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded. Optional modules: psutil=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import keras\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---- Environment Introspection ----\n",
    "# Fail gracefully if optional deps missing\n",
    "try:\n",
    "    import psutil  # optional system stats\n",
    "except ImportError:\n",
    "    psutil = None\n",
    "\n",
    "print(\"‚úÖ Imports loaded. Optional modules: psutil={}\".format(psutil is not None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42db15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive: ../complete_drum_archive\n",
      "Run output: /Users/Gilby/Projects/MLAudioClassifier/ClassifiedArchive/run_20251005_152941\n",
      "Found 47339 candidate audio files.\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_01.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_02.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_03.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_04.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_05.wav\n",
      "Found 47339 candidate audio files.\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_01.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_02.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_03.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_04.wav\n",
      "  ‚Ä¢ Access/Access Virus - B/BassDrum_05.wav\n"
     ]
    }
   ],
   "source": [
    "# Configuration & File Discovery Utilities\n",
    "# ========================================\n",
    "\n",
    "# Mirror-mode enhancement: replicate original directory tree and place classified samples\n",
    "'# into per-instrument subfolders under each original directory. Low-confidence or non-target',''# predictions go into a \"misc\" subfolder. Pre-sorted leaves (directory name matches a target label)',''# are copied verbatim without reclassification subfolder nesting.',\n",
    "# NOTE: Added dynamic label mapping + optional external mapping file.\n",
    "\n",
    "# You can create a JSON file at ../models/label_mapping.json containing a list of class names\n",
    "\"\"\"Example label_mapping.json\",\"\"\"\n",
    "#[\n",
    "#  \"Agogo\", \"Bell\", \"Bongo\", ...\n",
    "#]\n",
    "\n",
    "# The notebook will fall back to generic class_<idx> names if mapping length < model outputs.\n",
    "\n",
    "# To restrict which classes get materialized to disk (e.g. only core drum kit pieces),\n",
    "'# set TARGET_LABELS = [\"Crash\",\"Hihat\",\"Kick\",\"Ride\",\"Snare\",\"Tom\"] (case-sensitive).',\n",
    "from collections import defaultdict\n",
    "\n",
    "DEFAULT_ARCHIVE_PATH = Path('../complete_drum_archive')  # adjust if needed\n",
    "DEFAULT_OUTPUT_ROOT = Path('../ClassifiedArchive')\n",
    "MODELS_DIR = Path('../models')\n",
    "\n",
    "# Supported formats prioritized by typical drum sample usage\n",
    "SUPPORTED_FORMATS = [\".wav\", \".flac\", \".aiff\", \".aif\", \".mp3\"]  # remove \".mp3\" to avoid audioread fallback warnings\n",
    "DISABLE_MP3 = False  # set True to skip mp3 entirely (avoid audioread path)\n",
    "MAX_FILES_PER_RUN = None          # None = process all\n",
    "BATCH_SIZE = 32                   # in-memory inference batch size\n",
    "N_MFCC = 40\n",
    "TARGET_SR = 44100\n",
    "TARGET_SAMPLES = 50_000\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "NORMALIZE = True\n",
    "THREAD_WORKERS = 8                # for parallel feature extraction (future enhancement hook)\n",
    "COPY_MODE = \"copy\"                # one of: copy | symlink | none\n",
    "PRESERVE_STRUCTURE = True         # legacy filename embedding (ignored if MIRROR_STRUCTURE=True)\n",
    "MIRROR_STRUCTURE = True           # replicate directory tree and classify into subfolders\n",
    "DEDUP_HASH = True                 # skip files with identical content hash\n",
    "MIN_DURATION_SEC = 0.05           # skip extremely short blips\n",
    "MISC_CONFIDENCE_THRESHOLD = 0.50  # confidence below this -> misc/\n",
    "CONFIDENCE_THRESHOLD = 0.0        # absolute floor for recording prediction (keep 0.0)\n",
    "MISC_LABEL_NAME = 'misc'\n",
    "LABEL_MAP_FILE = MODELS_DIR / 'label_mapping.json'  # optional; list of label names\n",
    "TARGET_LABELS = [\"Crash\",\"Hihat\",\"Kick\",\"Ride\",\"Snare\",\"Tom\"]  # subset to actually emit; set to None to allow all\n",
    "CACHE_DIR = Path('.cache/archive_classifier')\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Legacy constant kept for backward compatibility; will be overwritten dynamically if mapping found\n",
    "INSTRUMENT_NAMES = [\"Crash\", \"Hihat\", \"Kick\", \"Ride\", \"Snare\", \"Tom\"]  # fallback subset\n",
    "\n",
    "ARCHIVE_PATH = DEFAULT_ARCHIVE_PATH\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_OUTPUT_DIR = (DEFAULT_OUTPUT_ROOT / f'run_{TIMESTAMP}').resolve()\n",
    "(RUN_OUTPUT_DIR / 'metadata').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Archive: {ARCHIVE_PATH}\")\n",
    "print(f\"Run output: {RUN_OUTPUT_DIR}\")\n",
    "\n",
    "def discover_audio_files(archive_path: Path, formats: List[str], max_files=None) -> List[Path]:\n",
    "    \"\"\"Discover audio files with optional limit.\"\"\"\n",
    "    files: List[Path] = []\n",
    "    for ext in formats:\n",
    "        if DISABLE_MP3 and ext.lower() == '.mp3':\n",
    "            continue\n",
    "        files.extend(archive_path.rglob(f'*{ext}'))\n",
    "    files.sort()\n",
    "    if max_files is not None and len(files) > max_files:\n",
    "        files = files[:max_files]\n",
    "    return files\n",
    "\n",
    "def filter_candidates(paths: List[Path]) -> List[Path]:\n",
    "    filtered = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            if p.stat().st_size == 0:\n",
    "                continue\n",
    "            filtered.append(p)\n",
    "        except OSError:\n",
    "            continue\n",
    "    return filtered\n",
    "\n",
    "audio_files = filter_candidates(discover_audio_files(ARCHIVE_PATH, SUPPORTED_FORMATS, MAX_FILES_PER_RUN))\n",
    "print(f\"Found {len(audio_files)} candidate audio files.\")\n",
    "for preview in audio_files[:5]:\n",
    "    print('  ‚Ä¢', preview.relative_to(ARCHIVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b4f6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: ../models/model1.keras\n",
      "Generating generic class_<idx> mapping.\n",
      "Model outputs 34 classes.\n",
      "First labels: ['class_0', 'class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6', 'class_7', 'class_8', 'class_9']\n",
      "Model input shape: (None, 40, 98)\n",
      "System Memory: 17.18 GB total / 4.45 GB free\n"
     ]
    }
   ],
   "source": [
    "# Model Loading & Feature Extraction\n",
    "# ==================================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='librosa')  # silence repetitive backend fallbacks\n",
    "\n",
    "def load_latest_model(model_dir: Path, pattern='model1.keras') -> keras.Model:\n",
    "    candidates = sorted(model_dir.glob(pattern))\n",
    "    if not candidates:\n",
    "        # fallback: any *.keras\n",
    "        candidates = sorted(model_dir.glob('*.keras'))\n",
    "    candidates = sorted(candidates, key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No model files in {model_dir}\")\n",
    "    print(f\"Loading model: {candidates[0]}\")\n",
    "    return keras.models.load_model(candidates[0])\n",
    "\n",
    "def load_label_mapping(model_obj: keras.Model) -> List[str]:\n",
    "    # Try explicit mapping file first\n",
    "    if LABEL_MAP_FILE.exists():\n",
    "        try:\n",
    "            data = json.loads(LABEL_MAP_FILE.read_text())\n",
    "            if isinstance(data, list) and len(data) == model_obj.output_shape[-1]:\n",
    "                print(f\"Using label mapping from {LABEL_MAP_FILE}\")\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è label_mapping.json length mismatch ({len(data)} vs {model_obj.output_shape[-1]}). Ignoring.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to parse label mapping file: {e}\")\n",
    "    # Fall back to INSTRUMENT_NAMES subset if compatible\n",
    "    out_dim = model_obj.output_shape[-1]\n",
    "    if len(INSTRUMENT_NAMES) == out_dim:\n",
    "        print(\"Using fallback INSTRUMENT_NAMES as full mapping.\")\n",
    "        return INSTRUMENT_NAMES\n",
    "    # Generic numbered classes\n",
    "    print(\"Generating generic class_<idx> mapping.\")\n",
    "    return [f'class_{i}' for i in range(out_dim)]\n",
    "\n",
    "def hash_file(path: Path, block_size=65536) -> str:\n",
    "    hasher = hashlib.md5()\n",
    "    with path.open('rb') as f:\n",
    "        for chunk in iter(lambda: f.read(block_size), b''):\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def safe_load_audio(path: Path):\n",
    "    \"\"\"Robust loader: try soundfile first, fallback to librosa/audioread; return (y, sr) or raise.\"\"\"\n",
    "    try:\n",
    "        y, sr = sf.read(path)\n",
    "        if y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        if sr != TARGET_SR:\n",
    "            y = librosa.resample(y, orig_sr=sr, target_sr=TARGET_SR)\n",
    "            sr = TARGET_SR\n",
    "        return y, sr\n",
    "    except Exception:\n",
    "        # fallback to librosa unified loader\n",
    "        y, sr = librosa.load(path, sr=TARGET_SR, mono=True)\n",
    "        return y, sr\n",
    "\n",
    "def load_and_preprocess(path: Path) -> Tuple[np.ndarray, Dict[str, Any]]:\n",
    "    \"\"\"Load audio and compute MFCC feature tensor plus metadata.\"\"\"\n",
    "    try:\n",
    "        y, sr = safe_load_audio(path)\n",
    "        if len(y) < MIN_DURATION_SEC * sr:\n",
    "            return None, {'error': 'too_short'}\n",
    "        if len(y) < TARGET_SAMPLES:\n",
    "            y = librosa.util.fix_length(y, size=TARGET_SAMPLES)\n",
    "        else:\n",
    "            y = y[:TARGET_SAMPLES]\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        if NORMALIZE:\n",
    "            mfcc = librosa.util.normalize(mfcc)\n",
    "        return mfcc, {'error': None, 'orig_sr': sr}\n",
    "    except Exception as e:\n",
    "        return None, {'error': str(e)}\n",
    "\n",
    "def batch_tensorize(feature_list: List[np.ndarray]) -> np.ndarray:\n",
    "    # shape (batch, n_mfcc, time) -> expand to (batch, n_mfcc, time, 1) if needed by model\n",
    "    arr = np.stack(feature_list, axis=0)\n",
    "    if arr.ndim == 3:\n",
    "        arr = arr[..., np.newaxis]\n",
    "    return arr\n",
    "\n",
    "model = load_latest_model(MODELS_DIR)\n",
    "LABELS = load_label_mapping(model)\n",
    "print(f\"Model outputs {len(LABELS)} classes.\")\n",
    "print(\"First labels:\", LABELS[:10])\n",
    "INPUT_SHAPE = model.input_shape\n",
    "print('Model input shape:', INPUT_SHAPE)\n",
    "\n",
    "# Optional device memory stats\n",
    "if psutil:\n",
    "    vm = psutil.virtual_memory()\n",
    "    print(f\"System Memory: {vm.total/1e9:.2f} GB total / {vm.available/1e9:.2f} GB free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b72e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 47339 files in 1480 batches of up to 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 702/1480 [01:50<01:42,  7.62it/s]/var/folders/pn/lx0_1y5d0bl2xdvpy5nkt0ph0000gn/T/ipykernel_1148/3596588520.py:58: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=TARGET_SR, mono=True)\n",
      "/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Exception ignored in: <function CFObject.__del__ at 0x13143cf40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/audioread/macca.py\", line 135, in __del__\n",
      "    _corefoundation.CFRelease(self._obj)\n",
      "AttributeError: 'CFURL' object has no attribute '_obj'\n",
      "Exception ignored in: <function ExtAudioFile.__del__ at 0x13143d940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/audioread/macca.py\", line 336, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/audioread/macca.py\", line 330, in close\n",
      "    if not self.closed:\n",
      "AttributeError: 'ExtAudioFile' object has no attribute 'closed'\n",
      "/var/folders/pn/lx0_1y5d0bl2xdvpy5nkt0ph0000gn/T/ipykernel_1148/3596588520.py:58: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(path, sr=TARGET_SR, mono=True)\n",
      "/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Exception ignored in: <function CFObject.__del__ at 0x13143cf40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/audioread/macca.py\", line 135, in __del__\n",
      "    _corefoundation.CFRelease(self._obj)\n",
      "AttributeError: 'CFURL' object has no attribute '_obj'\n",
      "Exception ignored in: <function ExtAudioFile.__del__ at 0x13143d940>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/audioread/macca.py\", line 336, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/Gilby/Projects/MLAudioClassifier/.venv/lib/python3.13/site-packages/audioread/macca.py\", line 330, in close\n",
      "    if not self.closed:\n",
      "AttributeError: 'ExtAudioFile' object has no attribute 'closed'\n",
      "Classifying: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1480/1480 [03:51<00:00,  6.39it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  Classification complete in 231.76s (204.3 files/sec)\n",
      "‚úîÔ∏è Updated hash cache: .cache/archive_classifier/seen_hashes.txt\n",
      "Class distribution (emitted subset):\n",
      "  class_10             9079\n",
      "  class_0              7128\n",
      "  class_12             6421\n",
      "  class_3              6241\n",
      "  class_9              2531\n",
      "  class_13             1623\n",
      "  class_6              1520\n",
      "  class_23             1445\n",
      "  class_11             1305\n",
      "  class_17             1183\n",
      "  class_1              881\n",
      "  class_5              859\n",
      "  class_20             737\n",
      "  class_27             643\n",
      "  class_28             585\n",
      "  class_26             545\n",
      "  class_30             449\n",
      "  class_7              411\n",
      "  class_33             357\n",
      "  class_14             357\n",
      "  class_4              350\n",
      "  class_25             336\n",
      "  class_15             244\n",
      "  class_16             228\n",
      "  class_2              202\n",
      "  class_21             182\n",
      "  class_32             175\n",
      "  class_19             161\n",
      "  class_24             145\n",
      "  class_31             105\n",
      "  class_29             80\n",
      "  class_22             33\n",
      "  class_18             24\n",
      "  class_8              1\n",
      "Errors: 773 | Successful (emitted or recorded): 46566 | Total recorded: 46566\n",
      "Top error causes:\n",
      "  too_short                 772\n",
      "  'PosixPath' object has no attribute 'encode' 1\n",
      "üìÑ Saved summary + 46566 detailed records -> /Users/Gilby/Projects/MLAudioClassifier/ClassifiedArchive/run_20251005_152941/metadata\n",
      "üìÑ Saved summary + 46566 detailed records -> /Users/Gilby/Projects/MLAudioClassifier/ClassifiedArchive/run_20251005_152941/metadata\n"
     ]
    }
   ],
   "source": [
    "# Optimized Parallel Classification & Output\n",
    "# ==========================================\n",
    "\n",
    "results: List[Dict[str, Any]] = []\n",
    "errors: List[Dict[str, Any]] = []\n",
    "error_causes: Dict[str,int] = {}\n",
    "hash_cache_path = CACHE_DIR / 'seen_hashes.txt'\n",
    "seen_hashes = set()\n",
    "if DEDUP_HASH and hash_cache_path.exists():\n",
    "    seen_hashes.update(h.strip() for h in hash_cache_path.read_text().splitlines() if h.strip())\n",
    "\n",
    "def is_presorted_leaf(path: Path) -> bool:\n",
    "    \"\"\"A directory whose name already matches a target label (case-insensitive).\"\"\"\n",
    "    if TARGET_LABELS is None:\n",
    "        return False\n",
    "    return path.is_dir() and path.name.lower() in {t.lower() for t in TARGET_LABELS}\n",
    "\n",
    "def mirror_destination(src_file: Path, pred_label: str, conf: float) -> Path:\n",
    "    \"\"\"Return destination path under MIRROR_STRUCTURE rules.\"\"\"\n",
    "    relative_parent = src_file.parent.relative_to(ARCHIVE_PATH)\n",
    "    parent_out_dir = RUN_OUTPUT_DIR / relative_parent\n",
    "    # create parent mirror\n",
    "    parent_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # If parent is already a pre-sorted leaf, just drop file straight inside (no nested class folder)\n",
    "    if is_presorted_leaf(src_file.parent):\n",
    "        out_dir = parent_out_dir\n",
    "    else:\n",
    "        # classification subfolder or misc\n",
    "        out_dir = parent_out_dir / pred_label.lower()\n",
    "        out_dir.mkdir(exist_ok=True)\n",
    "    base = src_file.name\n",
    "    name_no_ext, ext = os.path.splitext(base)\n",
    "    new_name = f\"{name_no_ext}__{pred_label.lower()}_{conf:.3f}{ext}\"\n",
    "    return out_dir / new_name\n",
    "\n",
    "def ensure_output_subdir(label: str) -> Path:\n",
    "    d = RUN_OUTPUT_DIR / label\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def emit_file(pred_label: str, conf: float, src: Path):\n",
    "    if MIRROR_STRUCTURE:\n",
    "        dst = mirror_destination(src, pred_label, conf)\n",
    "    else:\n",
    "        relative_tag = src.relative_to(ARCHIVE_PATH) if PRESERVE_STRUCTURE else src.name\n",
    "        relative_tag = str(relative_tag).replace('/', '_').replace('\\\\', '_')\n",
    "        out_name = f\"{pred_label.lower()}_{conf:.3f}_{relative_tag}\"\n",
    "        dst_dir = ensure_output_subdir(pred_label)\n",
    "        dst = dst_dir / out_name\n",
    "    if COPY_MODE == 'copy':\n",
    "        shutil.copy2(src, dst)\n",
    "    elif COPY_MODE == 'symlink':\n",
    "        if not dst.exists():\n",
    "            os.symlink(src, dst)\n",
    "    return dst\n",
    "\n",
    "def classify_batch(batch_paths: List[Path]) -> None:\n",
    "    feats = []\n",
    "    valid_paths = []\n",
    "    for p in batch_paths:\n",
    "        feat, meta = load_and_preprocess(p)\n",
    "        if feat is None:\n",
    "            errors.append({'file': str(p), 'error': meta['error']})\n",
    "            error_causes[meta['error']] = error_causes.get(meta['error'],0)+1\n",
    "            continue\n",
    "        feats.append(feat)\n",
    "        valid_paths.append(p)\n",
    "    if not feats:\n",
    "        return\n",
    "    X = batch_tensorize(feats)\n",
    "    probs = model.predict(X, verbose=0)\n",
    "    for i, p in enumerate(valid_paths):\n",
    "        prob_vec = probs[i]\n",
    "        label_idx = int(np.argmax(prob_vec))\n",
    "        conf = float(np.max(prob_vec))\n",
    "        pred_label = LABELS[label_idx] if label_idx < len(LABELS) else f'class_{label_idx}'\n",
    "        file_rec = {\n",
    "            'file': str(p),\n",
    "            'pred_label': pred_label,\n",
    "            'confidence': conf,\n",
    "            'probs': prob_vec.tolist(),\n",
    "            'hash': None\n",
    "        }\n",
    "        if DEDUP_HASH:\n",
    "            h = hash_file(p)\n",
    "            file_rec['hash'] = h\n",
    "            if h in seen_hashes:\n",
    "                file_rec['skipped_duplicate'] = True\n",
    "                results.append(file_rec)\n",
    "                continue\n",
    "            seen_hashes.add(h)\n",
    "        # Skip out-of-target for emission, but still record\n",
    "        if TARGET_LABELS is not None and pred_label not in TARGET_LABELS:\n",
    "            file_rec['filtered_out_of_target_set'] = True\n",
    "            # treat as misc candidate if MIRROR_STRUCTURE: we still store under misc\n",
    "            misc_label = MISC_LABEL_NAME\n",
    "            if MIRROR_STRUCTURE and COPY_MODE != 'none':\n",
    "                out_path = emit_file(misc_label, conf, p)\n",
    "                file_rec['output_path'] = str(out_path)\n",
    "                file_rec['relabelled_to_misc'] = True\n",
    "            results.append(file_rec)\n",
    "            continue\n",
    "        # Low confidence -> misc bucket (mirror mode)\n",
    "        emit_label = pred_label\n",
    "        if MIRROR_STRUCTURE and conf < MISC_CONFIDENCE_THRESHOLD:\n",
    "            emit_label = MISC_LABEL_NAME\n",
    "            file_rec['relabelled_low_conf_to_misc'] = True\n",
    "        if conf >= CONFIDENCE_THRESHOLD:\n",
    "            if COPY_MODE != 'none':\n",
    "                out_path = emit_file(emit_label, conf, p)\n",
    "                file_rec['output_path'] = str(out_path)\n",
    "        else:\n",
    "            file_rec['below_conf_threshold'] = True\n",
    "        results.append(file_rec)\n",
    "\n",
    "# Chunk audio files\n",
    "BATCHED = [audio_files[i:i+BATCH_SIZE] for i in range(0, len(audio_files), BATCH_SIZE)]\n",
    "print(f\"Processing {len(audio_files)} files in {len(BATCHED)} batches of up to {BATCH_SIZE}.\")\n",
    "\n",
    "start_time = time.time()\n",
    "for batch in tqdm(BATCHED, desc='Classifying'):\n",
    "    classify_batch(batch)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  Classification complete in {elapsed:.2f}s ({len(audio_files)/(elapsed+1e-9):.1f} files/sec)\")\n",
    "\n",
    "# Persist dedup hash cache\n",
    "if DEDUP_HASH:\n",
    "    with hash_cache_path.open('w') as f:\n",
    "        f.write('\\n'.join(sorted(seen_hashes)))\n",
    "    print(f\"‚úîÔ∏è Updated hash cache: {hash_cache_path}\")\n",
    "\n",
    "# Summaries (only target + misc if mirror mode)\n",
    "successful = [r for r in results if r.get('pred_label')]\n",
    "emitted = [r for r in results if 'output_path' in r]\n",
    "class_counts: Dict[str,int] = {}\n",
    "for r in emitted:\n",
    "    lbl = Path(r['output_path']).parent.name if MIRROR_STRUCTURE else r['pred_label']\n",
    "    class_counts[lbl] = class_counts.get(lbl, 0) + 1\n",
    "print(\"Emitted distribution (folder-based):\")\n",
    "for k,v in sorted(class_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {k:<20} {v}\")\n",
    "print(f\"Errors: {len(errors)} | Emitted: {len(emitted)} | Total examined: {len(audio_files)}\")\n",
    "if error_causes:\n",
    "    print(\"Top error causes:\")\n",
    "    for k,v in sorted(error_causes.items(), key=lambda x: -x[1])[:10]:\n",
    "        print(f\"  {k:<25} {v}\")\n",
    "\n",
    "# Save metadata\n",
    "summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'archive_path': str(ARCHIVE_PATH.resolve()),\n",
    "    'run_output_dir': str(RUN_OUTPUT_DIR),\n",
    "    'total_examined': len(audio_files),\n",
    "    'emitted': len(emitted),\n",
    "    'errors': len(errors),\n",
    "    'error_breakdown': error_causes,\n",
    "    'emitted_distribution': class_counts,\n",
    "    'config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'n_mfcc': N_MFCC,\n",
    "        'target_sr': TARGET_SR,\n",
    "        'target_samples': TARGET_SAMPLES,\n",
    "        'copy_mode': COPY_MODE,\n",
    "        'mirror_structure': MIRROR_STRUCTURE,\n",
    "        'misc_confidence_threshold': MISC_CONFIDENCE_THRESHOLD,\n",
    "        'misc_label': MISC_LABEL_NAME,\n",
    "        'preserve_structure': PRESERVE_STRUCTURE,\n",
    "        'dedup_hash': DEDUP_HASH,\n",
    "        'confidence_threshold': CONFIDENCE_THRESHOLD,\n",
    "        'target_labels_subset': TARGET_LABELS,\n",
    "        'label_space_size': len(LABELS)\n",
    "    }\n",
    "}\n",
    "\n",
    "meta_dir = RUN_OUTPUT_DIR / 'metadata'\n",
    "meta_dir.mkdir(exist_ok=True, parents=True)\n",
    "with (meta_dir / 'summary.json').open('w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "with (meta_dir / 'results.json').open('w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "if errors:\n",
    "    with (meta_dir / 'errors.json').open('w') as f:\n",
    "        json.dump(errors, f, indent=2)\n",
    "print(f\"üìÑ Saved summary + {len(results)} detailed records -> {meta_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
