{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Evaluation\n",
    "\n",
    "## 5. Load and Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for evaluation\n",
    "test_data_path = PATHS['data'] / 'mfcc_test_data.json'\n",
    "\n",
    "if not test_data_path.exists():\n",
    "    print(f\"⚠️  Test data not found: {test_data_path}\")\n",
    "    print(\"   Evaluation will be skipped\")\n",
    "    has_test_data = False\n",
    "else:\n",
    "    print(f\"Loading test data from: {test_data_path}\")\n",
    "    with open(test_data_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    X_test = np.array(test_data['mfcc'])\n",
    "    y_test = np.array(test_data['label'])\n",
    "    \n",
    "    # Normalize test labels to match training\n",
    "    unique_test_labels = np.unique(y_test)\n",
    "    test_label_mapping = {old: i for i, old in enumerate(sorted(unique_test_labels))}\n",
    "    y_test = np.array([test_label_mapping[v] for v in y_test])\n",
    "    \n",
    "    print(f\"✅ Loaded test set: {X_test.shape}\")\n",
    "    print(f\"   Samples: {X_test.shape[0]}\")\n",
    "    print(f\"   Classes: {len(np.unique(y_test))}\")\n",
    "    has_test_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "models_dict = {}\n",
    "\n",
    "model1_path = PATHS['models'] / 'model1.keras'\n",
    "if model1_path.exists():\n",
    "    print(f\"Loading Model 1 from: {model1_path}\")\n",
    "    models_dict['model1'] = keras.models.load_model(model1_path)\n",
    "    print(f\"✅ Model 1 loaded - Output shape: {models_dict['model1'].output_shape}\")\n",
    "else:\n",
    "    print(f\"⚠️  Model 1 not found: {model1_path}\")\n",
    "\n",
    "model2_path = PATHS['models'] / 'model2.keras'\n",
    "if model2_path.exists():\n",
    "    print(f\"\\nLoading Model 2 from: {model2_path}\")\n",
    "    models_dict['model2'] = keras.models.load_model(model2_path)\n",
    "    print(f\"✅ Model 2 loaded - Output shape: {models_dict['model2'].output_shape}\")\n",
    "else:\n",
    "    print(f\"⚠️  Model 2 not found: {model2_path}\")\n",
    "\n",
    "if not models_dict:\n",
    "    print(\"\\n❌ No trained models found. Run training cells first.\")\n",
    "else:\n",
    "    print(f\"\\n✅ Loaded {len(models_dict)} model(s) for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Metrics & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on test set\n",
    "if has_test_data and models_dict:\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Evaluating {model_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred_probs = model.predict(X_test, verbose=0)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_probs\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✅ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"   Correct: {(y_pred == y_test).sum()}/{len(y_test)}\")\n",
    "        print(f\"   Incorrect: {(y_pred != y_test).sum()}/{len(y_test)}\")\n",
    "    \n",
    "    # Compare models if multiple\n",
    "    if len(results) > 1:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Model Comparison\")\n",
    "        print(f\"{'='*60}\")\n",
    "        for model_name, res in results.items():\n",
    "            print(f\"{model_name:10s}: {res['accuracy']:.4f} ({res['accuracy']*100:.2f}%)\")\n",
    "        \n",
    "        best_model = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        print(f\"\\n🏆 Best model: {best_model[0]} ({best_model[1]['accuracy']*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping evaluation - no test data or models available\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "if len(results) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    model_names = list(results.keys())\n",
    "    accuracies = [results[m]['accuracy'] for m in model_names]\n",
    "    \n",
    "    bars = ax.bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e'][:len(model_names)])\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Model Comparison on Test Set', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.4f}\\n({acc*100:.2f}%)',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PATHS['results'] / 'model_comparison.png', dpi=120, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✅ Comparison plot saved to: {PATHS['results'] / 'model_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrices for each model\n",
    "if has_test_data and results:\n",
    "    # Get class names from test data\n",
    "    if 'label_map' in test_data:\n",
    "        class_names = [k for k, v in sorted(test_data['label_map'].items(), key=lambda x: x[1])]\n",
    "    else:\n",
    "        class_names = [f\"Class_{i}\" for i in range(len(np.unique(y_test)))]\n",
    "    \n",
    "    for model_name, res in results.items():\n",
    "        print(f\"\\nGenerating confusion matrix for {model_name}...\")\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_test, res['predictions'])\n",
    "        \n",
    "        # Normalize by true labels (rows)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Raw counts\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        ax1.set_title(f'{model_name} - Raw Counts')\n",
    "        ax1.set_ylabel('True Label')\n",
    "        ax1.set_xlabel('Predicted Label')\n",
    "        \n",
    "        # Normalized\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=ax2,\n",
    "                   xticklabels=class_names, yticklabels=class_names, vmin=0, vmax=1)\n",
    "        ax2.set_title(f'{model_name} - Normalized')\n",
    "        ax2.set_ylabel('True Label')\n",
    "        ax2.set_xlabel('Predicted Label')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PATHS['results'] / f'{model_name}_confusion_matrix.png', dpi=120, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✅ Saved: {PATHS['results'] / f'{model_name}_confusion_matrix.png'}\")\n",
    "        \n",
    "        # Print classification report\n",
    "        print(f\"\\nClassification Report for {model_name}:\")\n",
    "        print(classification_report(y_test, res['predictions'], target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
